{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN Cheat Sheet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bundickm/CheatSheets/blob/master/NN_Cheat_Sheet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwNDa6Qd-LxT",
        "colab_type": "text"
      },
      "source": [
        "# Documentation\n",
        "[Keras](https://keras.io/)\n",
        "\n",
        "[SpaCy](https://spacy.io/api/doc)\n",
        "\n",
        "[NLTK](https://www.nltk.org/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BX5bHVBoe8F",
        "colab_type": "text"
      },
      "source": [
        "# Definitions\n",
        "**Input Layer (Visible Layer)** - The layer is composed of artificial input neurons, and brings the initial data into the system for further processing by subsequent layers of artificial neurons. The input layer is the very beginning of the workflow.\n",
        "\n",
        "**Hidden Layer** - A layer in between the input layer and output layer, where artificial neurons take in a set of weighted inputs and produce an output through an activation function.\n",
        "\n",
        "**Output Layer** - The purpose of the output layer is to output a vector of values that is in a format that is suitable for the type of problem being addressed. Typically the output value is modified by an \"activation function\" to transform it into a format that makes sense in the context of that problem.\n",
        "\n",
        "**Neuron** - The elementary unit in an artificial neural network. The neuron receives one or more inputs and sums them to produce an output (or activation). Usually each input is separately weighted, and the sum is passed through a non-linear function known as an activation function or transfer function. The transfer functions usually have a sigmoid shape, but they may also take the form of other non-linear functions.\n",
        "\n",
        "**Weight** -  The strength or amplitude of a connection between two nodes. This is similar to slope in linear regression, where a weight is multiplied to the input to add up to form the output. Weights are numerical parameters which determine how strongly each of the neurons affects the other.\n",
        "\n",
        "**Activation Function** - The function that decides the output of a node given an input or set of inputs, whether a neuron should be activated or not by calculating weighted sum and further adding bias with it.\n",
        "\n",
        "**Node Map:** - A visual diagram of the architecture(topology) of a neural network. Like a flow chart it shows the path from inputs to outputs. They are usually color coded and help show, at a very high level, some of the differences in architecture between kinds of neural networks.\n",
        "\n",
        "**Perceptron** - A single node or neuron of a neural network with nothing else. It can take any number of inputs and spit out an output. Perceptrons can only fit linear boundaries between classes.\n",
        "\n",
        "**Feedforward Neural Network** - A Neural network made up of multiple perceptrons that has at least 1 hidden layer (does not include input and output layers).\n",
        "\n",
        "**Epoch** - one cycle of passing data forward through the network, measuring error given our specified cost function, and then via gradient descent, updating weights within the network to improve the quality of the predictions on the next iteration.\n",
        "\n",
        "**Backpropagation** - A neural network propagates the signal of the input data forward through its parameters towards the moment of decision, and then backpropagates information about the error, in reverse through the network, so that it can alter the parameters. This happens step by step:\n",
        "- The network makes a guess about data, using its parameters\n",
        "- The networkâ€™s is measured with a loss function\n",
        "- The error is backpropagated to adjust the wrong-headed parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZE6sUMVof86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# simple perceptron\n",
        "\n",
        "# define the sigmoid function to use as our activation function\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "# derivative to find slope at a given point\n",
        "def sigmoid_derivate(x):\n",
        "  sx = sigmoid(x)\n",
        "  return sx * (1-sx)\n",
        "\n",
        "# random starting weights to add\n",
        "weights = np.random.random((3,1))\n",
        "\n",
        "for iteration in range(100000):\n",
        "  # Weighted sum of inputs/weights\n",
        "  weighted_sum = np.dot(inputs, weights)\n",
        "  # Activate - where we are on the sigmoid graph\n",
        "  activated_output = sigmoid(weighted_sum)\n",
        "  # Calculate error\n",
        "  error = ground_truth - activated_output\n",
        "  # Adjust up or down by error amount using the slope of our current position \n",
        "  adjustments = error * sigmoid_derivate(activated_output)\n",
        "  # Calculate new weights based on our adjustments\n",
        "  weights += np.dot(inputs.T, adjustments)\n",
        "\n",
        "activated_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_2KcAv1rhHX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Simple NN with Backpropagation\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self):\n",
        "        #set up architecture of NN\n",
        "        self.input = 2\n",
        "        self.hiddenNodes = 3\n",
        "        self.outputNodes = 1\n",
        "        \n",
        "        #initial weights\n",
        "        self.weights1 = np.random.randn(self.input, self.hiddenNodes)\n",
        "        self.weights2 = np.random.randn(self.hiddenNodes, self.outputNodes)\n",
        "        \n",
        "    def sigmoid(self, s):\n",
        "        return 1 / (1+np.exp(-s))\n",
        "    \n",
        "    def sigmoidPrime(self, s):\n",
        "        return s * (1 - s)\n",
        "    \n",
        "    def feed_forward(self, X):\n",
        "        '''calculate the NN inference using feed forward'''\n",
        "        \n",
        "        #weighted sum of inputs and hidden\n",
        "        self.hidden_sum = np.dot(X, self.weights1)\n",
        "            \n",
        "        #activation of weighted sum\n",
        "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
        "          \n",
        "        #weighted sum between hidden and output\n",
        "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
        "        \n",
        "        #final activation\n",
        "        self.activated_output = self.sigmoid(self.output_sum)\n",
        "            \n",
        "        return self.activated_output\n",
        "    \n",
        "    def backward(self, X, y, o):\n",
        "        '''backward propagate through the network'''\n",
        "        self.o_error = y-o #error in output\n",
        "        self.o_delta = self.o_error * self.sigmoidPrime(o) #apply derivative of sigmoid to error\n",
        "        \n",
        "        self.z2_error = self.o_delta.dot(self.weights2.T) #z2 error: how much our hidden layer weights were off\n",
        "        self.z2_delta = self.z2_error*self.sigmoidPrime(self.activated_hidden)\n",
        "        \n",
        "        self.weights1 += X.T.dot(self.z2_delta) #adjust first set(input => hidden) weights\n",
        "        self.weights2 += self.activated_hidden.T.dot(self.o_delta) #adjust second set (hidden => output) weights\n",
        "        \n",
        "    def train(self, X, y):\n",
        "        o = self.feed_forward(X)\n",
        "        self.backward(X, y, o)\n",
        "\n",
        "\n",
        "nn = NeuralNetwork()\n",
        "for i in range(1000):\n",
        "    if (i+1 in [1,2,3,4,5]) or ((i+1) % 50 ==0):\n",
        "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
        "        print('Input: \\n', X)\n",
        "        print('Actual Output: \\n', y)\n",
        "        print('Predicted Output: \\n', str(nn.feed_forward(X)))\n",
        "        print(\"Loss: \\n\", str(np.mean(np.square(y - nn.feed_forward(X)))))\n",
        "    nn.train(X,y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NC_jiWZ2sWzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Keras with Hyperparameter Tuning\n",
        "\n",
        "import numpy\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "numpy.random.seed(42)\n",
        "\n",
        "# load dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# Function to create model, required for KerasClassifier\n",
        "def create_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(12, input_dim=8, activation='relu'))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
        "\n",
        "# define the grid search parameters\n",
        "param_grid = {'batch_size': [10, 20, 40, 60, 80, 100],\n",
        "              'epochs': [20]}\n",
        "\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
        "grid_result = grid.fit(X, y)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}