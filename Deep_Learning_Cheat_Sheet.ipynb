{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning Cheat Sheet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bundickm/CheatSheets/blob/master/Deep_Learning_Cheat_Sheet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAWbSsS5Bs8m",
        "colab_type": "text"
      },
      "source": [
        "# Documenation\n",
        "[SpaCy](https://spacy.io/api/doc)\n",
        "\n",
        "[NLTK](https://www.nltk.org/)\n",
        "\n",
        "[Keras](https://keras.io/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZMaBJbB13Bx",
        "colab_type": "text"
      },
      "source": [
        "#RNN's"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V_wCggxyfl-",
        "colab_type": "text"
      },
      "source": [
        "**Recurrent Neural Network** - a type of artificial neural network commonly used in speech recognition and natural language processing. RNNs are designed to recognize a data's sequential characteristics and use patterns to predict the next likely scenario.\n",
        "\n",
        "**Long Short Term Memory** - a kind of RNN, capable of learning long-term dependencies.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jaVDJrkwfiK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "#Trains an LSTM model on the IMDB sentiment classification task.\n",
        "The dataset is actually too small for LSTM to be of any advantage\n",
        "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
        "**Notes**\n",
        "- RNNs are tricky. Choice of batch size is important,\n",
        "choice of loss and optimizer is critical, etc.\n",
        "Some configurations won't converge.\n",
        "- LSTM loss decrease patterns during training can be quite different\n",
        "from what you see with CNNs/MLPs/etc.\n",
        "'''\n",
        "from __future__ import print_function\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.datasets import imdb\n",
        "\n",
        "max_features = 20000\n",
        "# cut texts after this number of words (among top max_features most common words)\n",
        "maxlen = 80\n",
        "batch_size = 32\n",
        "\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print('Train...')\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=15,\n",
        "          validation_data=(x_test, y_test))\n",
        "score, acc = model.evaluate(x_test, y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3od10uI02DM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "\n",
        "shakes = open('shakespear.txt')\n",
        "lines = shakes.readlines()\n",
        "lines[:5]\n",
        "\n",
        "\n",
        "line_list = []\n",
        "\n",
        "# Remove markdown, numbers, and blank lines\n",
        "for line in lines:\n",
        "    line = line.replace('\\n', '')\n",
        "    line = re.sub(r'(\\d)+','',line)\n",
        "    line = line.strip()\n",
        "\n",
        "    line_list.append(line)\n",
        "\n",
        "# Remove blank lines\n",
        "line_list = list(filter(None, line_list))\n",
        "\n",
        "# Remove first line\n",
        "shakes = line_list[1:]\n",
        "\n",
        "text = ' '.join(shakes[0:100])\n",
        "\n",
        "\n",
        "# split and remove duplicate characters. convert to list.\n",
        "chars = list(set(text)) \n",
        "\n",
        "# the number of unique characters\n",
        "num_chars = len(chars) \n",
        "txt_data_size = len(text)\n",
        "\n",
        "print(\"unique characters : \", num_chars)\n",
        "print(\"txt_data_size : \", txt_data_size)\n",
        "\n",
        "\n",
        "# one hot encode\n",
        "# \"enumerate\" returns index and value. Convert it to dictionary\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars)) \n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "print(char_to_int)\n",
        "print(\"----------------------------------------------------\")\n",
        "print(int_to_char)\n",
        "print(\"----------------------------------------------------\")\n",
        "# integer encode input data\n",
        "# \"integer_encoded\" is a list which has a sequence converted from an original data to integers.\n",
        "integer_encoded = [char_to_int[i] for i in text] \n",
        "print(integer_encoded)\n",
        "print(\"----------------------------------------------------\")\n",
        "print(\"data length : \", len(integer_encoded))\n",
        "\n",
        "\n",
        "# hyperparameters\n",
        "iteration = 1000\n",
        "sequence_length = 40\n",
        "batch_size = round((txt_data_size /sequence_length)+0.5) # = math.ceil\n",
        "hidden_size = 500  # size of hidden layer of neurons.  \n",
        "learning_rate = 1e-1\n",
        "\n",
        "# model parameters\n",
        "W_xh = np.random.randn(hidden_size, num_chars)*0.01     # weight input -> hidden. \n",
        "W_hh = np.random.randn(hidden_size, hidden_size)*0.01   # weight hidden -> hidden\n",
        "W_hy = np.random.randn(num_chars, hidden_size)*0.01     # weight hidden -> output\n",
        "\n",
        "b_h = np.zeros((hidden_size, 1)) # hidden bias\n",
        "b_y = np.zeros((num_chars, 1)) # output bias\n",
        "\n",
        "h_prev = np.zeros((hidden_size,1)) # h_(t-1)\n",
        "\n",
        "\n",
        "def forwardprop(inputs, targets, h_prev):\n",
        "        \n",
        "    # Since the RNN receives the sequence, the weights are not updated during one sequence.\n",
        "    xs, hs, ys, ps = {}, {}, {}, {} # dictionary\n",
        "    hs[-1] = np.copy(h_prev) # Copy previous hidden state vector to -1 key value.\n",
        "    loss = 0 # loss initialization\n",
        "    \n",
        "    for t in range(len(inputs)): # t is a \"time step\" and is used as a key(dic).  \n",
        "        \n",
        "        xs[t] = np.zeros((num_chars,1)) \n",
        "        xs[t][inputs[t]] = 1\n",
        "        hs[t] = np.tanh(np.dot(W_xh, xs[t]) + np.dot(W_hh, hs[t-1]) + b_h) # hidden state. \n",
        "        ys[t] = np.dot(W_hy, hs[t]) + b_y # unnormalized log probabilities for next chars\n",
        "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars. \n",
        "        \n",
        "        # Softmax. -> The sum of probabilities is 1 even without the exp() function, but all of the elements are positive through the exp() function.\n",
        "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss). Efficient and simple code\n",
        "\n",
        "    return loss, ps, hs, xs\n",
        "\n",
        "\n",
        "def backprop(ps, inputs, hs, xs, targets):\n",
        "\n",
        "    dWxh, dWhh, dWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy) # make all zero matrices.\n",
        "    dbh, dby = np.zeros_like(b_h), np.zeros_like(b_y)\n",
        "    dhnext = np.zeros_like(hs[0]) # (hidden_size,1) \n",
        "\n",
        "    # reversed\n",
        "    for t in reversed(range(len(inputs))):\n",
        "        dy = np.copy(ps[t]) # shape (num_chars,1).  \"dy\" means \"dloss/dy\"\n",
        "        dy[targets[t]] -= 1 # backprop into y. After taking the soft max in the input vector, subtract 1 from the value of the element corresponding to the correct label.\n",
        "        dWhy += np.dot(dy, hs[t].T)\n",
        "        dby += dy \n",
        "        dh = np.dot(W_hy.T, dy) + dhnext # backprop into h. \n",
        "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity #tanh'(x) = 1-tanh^2(x)\n",
        "        dbh += dhraw\n",
        "        dWxh += np.dot(dhraw, xs[t].T)\n",
        "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
        "        dhnext = np.dot(W_hh.T, dhraw)\n",
        "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]: \n",
        "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients.  \n",
        "    \n",
        "    return dWxh, dWhh, dWhy, dbh, dby\n",
        "\n",
        "\n",
        "data_pointer = 0\n",
        "\n",
        "# memory variables for Adagrad\n",
        "mWxh, mWhh, mWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy)\n",
        "mbh, mby = np.zeros_like(b_h), np.zeros_like(b_y) \n",
        "\n",
        "for i in range(iteration):\n",
        "    h_prev = np.zeros((hidden_size,1)) # reset RNN memory\n",
        "    data_pointer = 0 # go from start of data\n",
        "    \n",
        "    for b in range(batch_size):\n",
        "        inputs = [char_to_int[ch] for ch in text[data_pointer:data_pointer+sequence_length]]\n",
        "        targets = [char_to_int[ch] for ch in text[data_pointer+1:data_pointer+sequence_length+1]] # t+1        \n",
        "            \n",
        "        if (data_pointer+sequence_length+1 >= len(text) and b == batch_size-1): # processing of the last part of the input data. \n",
        "            targets.append(char_to_int[\" \"])   # When the data doesn't fit, add space(\" \") to the back.\n",
        "\n",
        "        # forward\n",
        "        loss, ps, hs, xs = forwardprop(inputs, targets, h_prev)\n",
        "        # backward\n",
        "        dWxh, dWhh, dWhy, dbh, dby = backprop(ps, inputs, hs, xs, targets) \n",
        "        \n",
        "        # perform parameter update with Adagrad\n",
        "        for param, dparam, mem in zip([W_xh, W_hh, W_hy, b_h, b_y], \n",
        "                                    [dWxh, dWhh, dWhy, dbh, dby], \n",
        "                                    [mWxh, mWhh, mWhy, mbh, mby]):\n",
        "            mem += dparam * dparam # elementwise\n",
        "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update      \n",
        "    \n",
        "        data_pointer += sequence_length # move data pointer\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print ('iter %d, loss: %f' % (i, loss)) # print progress\n",
        "\n",
        "\n",
        "predict('S', 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhknUCC-1jPg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Input, Model\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.wrappers import TimeDistributed\n",
        "import keras.callbacks\n",
        "import keras.backend as K\n",
        "import scipy.misc\n",
        "import json\n",
        "\n",
        "import os, sys\n",
        "import re\n",
        "\n",
        "from keras.optimizers import RMSprop\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.utils import get_file\n",
        "\n",
        "\n",
        "shakes = open('shakespear.txt')\n",
        "shakes = shakes.read()\n",
        "training_text = shakes.split('\\nTHE END', 1)[-1]\n",
        "training_text = training_text[:1000000]\n",
        "len(training_text)\n",
        "\n",
        "\n",
        "chars = list(sorted(set(lines)))\n",
        "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
        "len(chars)\n",
        "\n",
        "\n",
        "def char_rnn_model(num_chars, num_layers, num_nodes=512, dropout=.1):\n",
        "    input = Input(shape=(None, num_chars), name='input')\n",
        "    prev = input\n",
        "    for i in range(num_layers):\n",
        "        lstm = LSTM(num_nodes, return_sequences=True, name='lstm_layer_%d' % (i+1))(prev)\n",
        "        if dropout:\n",
        "            prev = Dropout(dropout)(lstm)\n",
        "        else:\n",
        "            prev = lstm\n",
        "    dense = TimeDistributed(Dense(num_chars, name='dense', activation='softmax'))(prev)\n",
        "    model = Model(inputs=[input], outputs=[dense])\n",
        "    optimizer = RMSprop(lr=.01)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "model = char_rnn_model(len(chars), num_layers=2, num_nodes=640, dropout=0)\n",
        "model.summary()\n",
        "\n",
        "\n",
        "CHUNK_SIZE = 160\n",
        "\n",
        "def data_generator(all_text, char_to_idx, batch_size, chunk_size):\n",
        "    X = np.zeros((batch_size, chunk_size, len(char_to_idx)))\n",
        "    y = np.zeros((batch_size, chunk_size, len(char_to_idx)))\n",
        "    while True:\n",
        "        for row in range(batch_size):\n",
        "            idx = random.randrange(len(all_text) - chunk_size -1)\n",
        "            chunk = np.zeros((chunk_size + 1, len(char_to_idx)))\n",
        "            for i in range(chunk_size + 1):\n",
        "                chunk[i, char_to_idx[all_text[idx + i]]] = 1\n",
        "            X[row, :, :] = chunk[:chunk_size]\n",
        "            y[row, :, :] = chunk[1:]\n",
        "        yield X, y\n",
        "\n",
        "next(data_generator(training_text, char_to_idx, 4, chunk_size=CHUNK_SIZE))\n",
        "\n",
        "\n",
        "early = keras.callbacks.EarlyStopping(monitor='loss',\n",
        "                                      min_delta=.03,\n",
        "                                      patience=3,\n",
        "                                      verbose=0, \n",
        "                                      mode='auto')\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "model.fit_generator(\n",
        "    data_generator(training_text, char_to_idx, batch_size=BATCH_SIZE, chunk_size=CHUNK_SIZE),\n",
        "    epochs=40,\n",
        "    callbacks=[early,],\n",
        "    steps_per_epoch=2 * len(training_text) / (BATCH_SIZE * CHUNK_SIZE),\n",
        "    verbose=2)\n",
        "\n",
        "\n",
        "def generate_output(model, training_text, start_index=None, diversity=None, amount=400):\n",
        "    if start_index is None:\n",
        "        start_index = random.randint(0, len(training_text) - CHUNK_SIZE - 1)\n",
        "    generated = training_text[start_index: start_index + CHUNK_SIZE]\n",
        "    yield generated + '#'\n",
        "    for i in range(amount):\n",
        "        x = np.zeros((1, len(generated), len(chars)))\n",
        "        for t, char in enumerate(generated):\n",
        "            x[0, t, char_to_idx[char]] = 1.\n",
        "        preds = model.predict(x, verbose=0)[0]\n",
        "        if diversity is None:\n",
        "            next_index = np.argmax(preds[len(generated) - 1])\n",
        "        else:\n",
        "            preds = np.asarray(preds[len(generated) - 1]).astype('float64')\n",
        "            preds = np.log(preds) / diversity\n",
        "            exp_preds = np.exp(preds)\n",
        "            preds = exp_preds / np.sum(exp_preds)\n",
        "            probas = np.random.multinomial(1, preds, 1)\n",
        "            next_index = np.argmax(probas)\n",
        "        next_char = chars[next_index]\n",
        "        yield next_char\n",
        "            \n",
        "        generated += next_char\n",
        "    return generated\n",
        "\n",
        "for ch in generate_output(model, training_text, amount=10):\n",
        "    sys.stdout.write(ch)\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSYXH0Pt19ki",
        "colab_type": "text"
      },
      "source": [
        "#CNN's"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fcf_Ju0W1_oe",
        "colab_type": "text"
      },
      "source": [
        "**Convolution** - an operation on two functions that produces a third function, showing how one function modifies another.\n",
        "\n",
        "**Convolutional Neural Network** - An algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image and be able to differentiate one from the other.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbCRbx9K1-_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "\n",
        "def process_img_path(img_path):\n",
        "  return image.load_img(img_path, target_size=(224, 224))\n",
        "\n",
        "def img_contains(img):\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  x = preprocess_input(x)\n",
        "  model = ResNet50(weights='imagenet')\n",
        "  features = model.predict(x)\n",
        "  results = decode_predictions(features, top=3)[0]\n",
        "  print(results)\n",
        "  for entry in results:\n",
        "    if entry[1] == 'banana':\n",
        "      return entry[2]\n",
        "  return 0.0\n",
        "\n",
        "\n",
        "Image(filename='./downloads/animal national park/10.Nairobi_National_Park%2C_Kenya_%2832570316676%29.jpg', width=600)\n",
        "\n",
        "\n",
        "img_contains(process_img_path('./downloads/animal national park/10.Nairobi_National_Park%2C_Kenya_%2832570316676%29.jpg'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Zu0X_EE36zC",
        "colab_type": "text"
      },
      "source": [
        "#GAN's"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rMVX1hx3-4m",
        "colab_type": "text"
      },
      "source": [
        "**Generative Adversarial Networks** - A class of machine learning systems in which two neural networks contest with each other in a game (in the sense of game theory, often but not always in the form of a zero-sum game). Given a training set, this technique learns to generate new data with the same statistics as the training set.\n",
        "- The discriminator is trained with real - but unlabeled - data, and has the goal of identifying whether or not some new item belongs in it.\n",
        "- The generator starts from noise (it doesn't see the data at all!), and tries to generate output to fool the discriminator (and gets to update based on feedback).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_Nw088C3tEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm  # performance timing\n",
        "\n",
        "# Building on Keras\n",
        "from keras.layers import Input\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers.core import Dense, Dropout\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.optimizers import Adam\n",
        "from keras import initializers\n",
        "\n",
        "\n",
        "np.random.seed(10)\n",
        "random_dim = 100\n",
        " \n",
        "def load_minst_data():\n",
        "    # load the data - we'll use Fashion MNIST, for a change of pace\n",
        "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "    # normalize our inputs to be in the range[-1, 1] \n",
        "    x_train = (x_train.astype(np.float32) - 127.5)/127.5\n",
        "    # convert x_train with a shape of (60000, 28, 28) to (60000, 784) so we have\n",
        "    # 784 columns per row\n",
        "    x_train = x_train.reshape(60000, 784)\n",
        "    return (x_train, y_train, x_test, y_test)\n",
        "\n",
        "\n",
        "def get_discriminator(optimizer):\n",
        "    discriminator = Sequential()\n",
        "    discriminator.add(Dense(\n",
        "        1024, input_dim=784,\n",
        "        kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
        "    discriminator.add(LeakyReLU(0.2))\n",
        "    discriminator.add(Dropout(0.3))\n",
        " \n",
        "    discriminator.add(Dense(512))\n",
        "    discriminator.add(LeakyReLU(0.2))\n",
        "    discriminator.add(Dropout(0.3))\n",
        " \n",
        "    discriminator.add(Dense(256))\n",
        "    discriminator.add(LeakyReLU(0.2))\n",
        "    discriminator.add(Dropout(0.3))\n",
        " \n",
        "    discriminator.add(Dense(1, activation='sigmoid'))\n",
        "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "    return discriminator\n",
        "\n",
        "def get_generator(optimizer):\n",
        "    generator = Sequential()\n",
        "    generator.add(Dense(\n",
        "        256, input_dim=random_dim,\n",
        "        kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
        "    generator.add(LeakyReLU(0.2))\n",
        " \n",
        "    generator.add(Dense(512))\n",
        "    generator.add(LeakyReLU(0.2))\n",
        " \n",
        "    generator.add(Dense(1024))\n",
        "    generator.add(LeakyReLU(0.2))\n",
        " \n",
        "    generator.add(Dense(784, activation='tanh'))\n",
        "    generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "    return generator\n",
        "\n",
        "def get_gan_network(discriminator, random_dim, generator, optimizer):\n",
        "    # We initially set trainable to False since we only want to train either the \n",
        "    # generator or discriminator at a time\n",
        "    discriminator.trainable = False\n",
        "    # gan input (noise) will be 100-dimensional vectors\n",
        "    gan_input = Input(shape=(random_dim,))\n",
        "    # the output of the generator (an image)\n",
        "    x = generator(gan_input)\n",
        "    # get the output of the discriminator (probability if the image is real/not)\n",
        "    gan_output = discriminator(x)\n",
        "    gan = Model(inputs=gan_input, outputs=gan_output)\n",
        "    gan.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "    return gan\n",
        "\n",
        "def plot_generated_images(epoch, generator, examples=100, dim=(10, 10),\n",
        "                          figsize=(10, 10)):\n",
        "    noise = np.random.normal(0, 1, size=[examples, random_dim])\n",
        "    generated_images = generator.predict(noise)\n",
        "    generated_images = generated_images.reshape(examples, 28, 28)\n",
        " \n",
        "    plt.figure(figsize=figsize)\n",
        "    for i in range(generated_images.shape[0]):\n",
        "        plt.subplot(dim[0], dim[1], i+1)\n",
        "        plt.imshow(generated_images[i], interpolation='nearest', cmap='gray_r')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('gan_generated_image_epoch_%d.png' % epoch)\n",
        "\n",
        "\n",
        "def train(epochs=1, batch_size=128):\n",
        "    # Get the training and testing data\n",
        "    x_train, y_train, x_test, y_test = load_minst_data()\n",
        "    # Split the training data into batches of size 128\n",
        "    batch_count = x_train.shape[0] // batch_size\n",
        " \n",
        "    # Build our GAN netowrk\n",
        "    adam = Adam(lr=0.0002, beta_1=0.5)\n",
        "    generator = get_generator(adam)\n",
        "    discriminator = get_discriminator(adam)\n",
        "    gan = get_gan_network(discriminator, random_dim, generator, adam)\n",
        " \n",
        "    for e in range(1, epochs+1):\n",
        "        print ('-'*15, 'Epoch %d' % e, '-'*15)\n",
        "        for _ in tqdm(range(batch_count)):\n",
        "            # Get a random set of input noise and images\n",
        "            noise = np.random.normal(0, 1, size=[batch_size, random_dim])\n",
        "            image_batch = x_train[np.random.randint(0, x_train.shape[0],\n",
        "                                                    size=batch_size)]\n",
        " \n",
        "            # Generate fake MNIST images\n",
        "            generated_images = generator.predict(noise)\n",
        "            X = np.concatenate([image_batch, generated_images])\n",
        " \n",
        "            # Labels for generated and real data\n",
        "            y_dis = np.zeros(2*batch_size)\n",
        "            # One-sided label smoothing\n",
        "            y_dis[:batch_size] = 0.9\n",
        " \n",
        "            # Train discriminator\n",
        "            discriminator.trainable = True\n",
        "            discriminator.train_on_batch(X, y_dis)\n",
        " \n",
        "            # Train generator\n",
        "            noise = np.random.normal(0, 1, size=[batch_size, random_dim])\n",
        "            y_gen = np.ones(batch_size)\n",
        "            discriminator.trainable = False\n",
        "            gan.train_on_batch(noise, y_gen)\n",
        " \n",
        "        if e == 1 or e % 20 == 0:\n",
        "            plot_generated_images(e, generator)\n",
        " \n",
        "train(40, 64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "012cYlHv56Pz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install emoji_data_python\n",
        "\n",
        "\n",
        "\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import color\n",
        "import glob\n",
        "\n",
        "\n",
        "#   Create Array   to-grayscale  read file              for everything in directory\n",
        "emojis = np.array([color.rgb2gray(imageio.imread(file)) for file in glob.glob('./emoji/*png')])\n",
        "\n",
        "emojis = emojis.reshape(861, 784)\n",
        "\n",
        "emojis.shape\n",
        "\n",
        "\n",
        "def get_discriminator(optimizer):\n",
        "    discriminator = Sequential()\n",
        "    discriminator.add(Dense(\n",
        "        1024, input_dim=784,\n",
        "        kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
        "    discriminator.add(LeakyReLU(0.2))\n",
        "    discriminator.add(Dropout(0.3))\n",
        " \n",
        "    discriminator.add(Dense(512))\n",
        "    discriminator.add(LeakyReLU(0.2))\n",
        "    discriminator.add(Dropout(0.3))\n",
        " \n",
        "    discriminator.add(Dense(256))\n",
        "    discriminator.add(LeakyReLU(0.2))\n",
        "    discriminator.add(Dropout(0.3))\n",
        " \n",
        "    discriminator.add(Dense(1, activation='sigmoid'))\n",
        "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "    return discriminator\n",
        "\n",
        "def get_generator(optimizer):\n",
        "    generator = Sequential()\n",
        "    generator.add(Dense(\n",
        "        256, input_dim=random_dim,\n",
        "        kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
        "    generator.add(LeakyReLU(0.2))\n",
        " \n",
        "    generator.add(Dense(512))\n",
        "    generator.add(LeakyReLU(0.2))\n",
        " \n",
        "    generator.add(Dense(1024))\n",
        "    generator.add(LeakyReLU(0.2))\n",
        " \n",
        "    generator.add(Dense(784, activation='tanh'))\n",
        "    generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "    return generator\n",
        "\n",
        "def get_gan_network(discriminator, random_dim, generator, optimizer):\n",
        "    # We initially set trainable to False since we only want to train either the \n",
        "    # generator or discriminator at a time\n",
        "    discriminator.trainable = False\n",
        "    # gan input (noise) will be 100-dimensional vectors\n",
        "    gan_input = Input(shape=(random_dim,))\n",
        "    # the output of the generator (an image)\n",
        "    x = generator(gan_input)\n",
        "    # get the output of the discriminator (probability if the image is real/not)\n",
        "    gan_output = discriminator(x)\n",
        "    gan = Model(inputs=gan_input, outputs=gan_output)\n",
        "    gan.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "    return gan\n",
        "\n",
        "def plot_generated_images(epoch, generator, examples=100, dim=(10, 10),\n",
        "                          figsize=(10, 10)):\n",
        "    noise = np.random.normal(0, 1, size=[examples, random_dim])\n",
        "    generated_images = generator.predict(noise)\n",
        "    generated_images = generated_images.reshape(examples, 28, 28)\n",
        " \n",
        "    plt.figure(figsize=figsize)\n",
        "    for i in range(generated_images.shape[0]):\n",
        "        plt.subplot(dim[0], dim[1], i+1)\n",
        "        plt.imshow(generated_images[i], interpolation='nearest', cmap='gray_r')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('gan_generated_image_epoch_%d.png' % epoch)\n",
        "\n",
        "\n",
        "%cd gan_epoch\n",
        "def train(epochs=1, batch_size=128):\n",
        "    \n",
        "    # Split the training data into batches of size 128\n",
        "    batch_count = emojis.shape[0] // batch_size\n",
        " \n",
        "    # Build our GAN netowrk\n",
        "    adam = Adam(lr=0.0002, beta_1=0.5)\n",
        "    generator = get_generator(adam)\n",
        "    discriminator = get_discriminator(adam)\n",
        "    gan = get_gan_network(discriminator, random_dim, generator, adam)\n",
        " \n",
        "    for e in range(1, epochs+1):\n",
        "        print ('-'*15, 'Epoch %d' % e, '-'*15)\n",
        "        for _ in tqdm(range(batch_count)):\n",
        "            # Get a random set of input noise and images\n",
        "            noise = np.random.normal(0, 1, size=[batch_size, random_dim])\n",
        "            image_batch = emojis[np.random.randint(0, emojis.shape[0],\n",
        "                                                    size=batch_size)]\n",
        " \n",
        "            # Generate fake MNIST images\n",
        "            generated_images = generator.predict(noise)\n",
        "            X = np.concatenate([image_batch, generated_images])\n",
        " \n",
        "            # Labels for generated and real data\n",
        "            y_dis = np.zeros(2*batch_size)\n",
        "            # One-sided label smoothing\n",
        "            y_dis[:batch_size] = 0.9\n",
        " \n",
        "            # Train discriminator\n",
        "            discriminator.trainable = True\n",
        "            discriminator.train_on_batch(X, y_dis)\n",
        " \n",
        "            # Train generator\n",
        "            noise = np.random.normal(0, 1, size=[batch_size, random_dim])\n",
        "            y_gen = np.ones(batch_size)\n",
        "            discriminator.trainable = False\n",
        "            gan.train_on_batch(noise, y_gen)\n",
        " \n",
        "        if e == 1 or e % 500 == 0:\n",
        "            plot_generated_images(e, generator)\n",
        " \n",
        "train(6000, 64)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}