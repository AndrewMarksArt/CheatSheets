{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Cheat Sheet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bundickm/CheatSheets/blob/master/NLP_Cheat_Sheet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlXh5gdsA67Z",
        "colab_type": "text"
      },
      "source": [
        "# Documentation\n",
        "\n",
        "[SpaCy](https://spacy.io/api/doc)\n",
        "\n",
        "[NLTK](https://www.nltk.org/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ9ja7LV-8tK",
        "colab_type": "text"
      },
      "source": [
        "**Natural Language Processing (NLP)**  - A subfield of computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data. \n",
        "\n",
        "**Token** - An instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing\n",
        "\n",
        "Atrributes of good tokens:\n",
        "- Should be stored in an iterable datastructure (Allows analysis of the \"semantic unit\")\n",
        "- Should be all the same case (Reduces the complexity of our data)\n",
        "- Should be free of non-alphanumeric characters (ie punctuation, whitespace)\n",
        "- Removes information that is probably not relevant to the analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbxhtZ4-kC6l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Really basic tokenizer.\n",
        "    Parses a string into a list of semantic units (words)\n",
        "    \"\"\"\n",
        "    tokens = re.sub(r'[^a-zA-Z ^0-9]', '', text).lower().split()\n",
        "    return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11HWyK-Ol1NU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Basic tokenizing with SpaCy\n",
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "\n",
        "nlp = en_core_web_sm.load()\n",
        "tokenizer = Tokenizer(nlp.vocab)\n",
        "[token.text for token in tokenizer(sample)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twDlSKEzmrGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenizer Pipe\n",
        "tokens = []\n",
        "\n",
        "for doc in tokenizer.pipe(df['reviews.text'], batch_size=500):\n",
        "    doc_tokens = [token.text for token in doc]\n",
        "    tokens.append(doc_tokens)\n",
        "    \n",
        "df['tokens'] = tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNRsAYMy_K9i",
        "colab_type": "text"
      },
      "source": [
        "**Stopwords** - Words which are filtered out before or after processing of natural language data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9AmiHminCct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Spacy's Default Stop Words\n",
        "nlp.Defaults.stop_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wiwe1khnTMB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Stop Words and Remove punctuation\n",
        "tokens = []\n",
        "\n",
        "for doc in tokenizer.pipe(df['reviews.text'], batch_size=500):\n",
        "    doc_tokens = []\n",
        "    for token in doc:\n",
        "        if (token.is_stop == False) and (token.is_punct == False):\n",
        "            doc_tokens.append(token.text.lower())\n",
        "    tokens.append(doc_tokens)\n",
        "    \n",
        "df['tokens'] = tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWx9AH7cnhFY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extending Stop Words\n",
        "STOP_WORDS = nlp.Defaults.stop_words.union(['I', 'amazon', 'i', 'it', \"it's\", 'it.', 'the', 'this',])\n",
        "\n",
        "tokens = []\n",
        "\n",
        "for doc in tokenizer.pipe(df['reviews.text'], batch_size=500):\n",
        "    doc_tokens = []\n",
        "\n",
        "    for token in doc: \n",
        "        if (token.text not in STOP_WORDS):\n",
        "            doc_tokens.append(token.text.lower())\n",
        "\n",
        "    tokens.append(doc_tokens)\n",
        "    \n",
        "df['tokens'] = tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZrz8hid_ZSB",
        "colab_type": "text"
      },
      "source": [
        "**Statistical Trimming** - A technique to preserve the words that describe most of the variation in your data.\n",
        "\n",
        "**Stemming** -  Usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\n",
        "\n",
        "**Lemmatization** - Usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75Je-WsQp5cI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Porter stemming algorithm\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "words = [\"game\",\"gaming\",\"gamed\",\"games\",\"gamer\"]\n",
        "\n",
        "for word in words:\n",
        "    print(ps.stem(word))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEyTm0EHqYvi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SpaCy lemmatization\n",
        "text = \"This is the start of our NLP adventure. We started here with Spacy.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    print(token.lemma_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsdIgPFCqmr-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Wrap it all in a function\n",
        "def get_lemmas(text):\n",
        "    lemmas = []\n",
        "    doc = nlp(text)\n",
        "    \n",
        "    for token in doc:\n",
        "        if ((token.is_stop == False) and (token.is_punct == False) and \n",
        "            (token.pos_ != 'PRON')):\n",
        "            doc_tokens.append(token.lemma_)\n",
        "\n",
        "    return lemmas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SJQy6We_wJp",
        "colab_type": "text"
      },
      "source": [
        "**Vectorization** - Transforming text into a meaningful vector (or array) of numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BwV6EU2q2A5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Count Vectorizer\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# list of text documents\n",
        "text = [\"We created a new dataset which emphasizes diversity of content, by \\\n",
        "        scraping content from the Internet.\",\" In order to preserve document \\\n",
        "        quality, we used only pages which have been curated/filtered by \\\n",
        "        humans—specifically, we used outbound links from Reddit which received \\\n",
        "        at least 3 karma.\",\" This can be thought of as a heuristic indicator \\\n",
        "        for whether other users found the link interesting (whether educational\\\n",
        "         or funny), leading to higher data quality than other similar datasets,\\\n",
        "         such as CommonCrawl.\"]\n",
        "\n",
        "# create the transform\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "\n",
        "# tokenize and build vocab\n",
        "vectorizer.fit(text)\n",
        "\n",
        "\n",
        "# Create a Vocabulary\n",
        "# The vocabulary establishes all of the possible words that we might use.\n",
        "vectorizer.vocabulary_\n",
        "\n",
        "# The vocabulary dictionary does not represent the counts of words!!\n",
        "dtm = vectorizer.transform(text)\n",
        "\n",
        "# Get Word Counts for each document\n",
        "dtm = pd.DataFrame(dtm.todense(), columns=vectorizer.get_feature_names())\n",
        "dtm.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGcJywhK_0Xz",
        "colab_type": "text"
      },
      "source": [
        "**Bag-of-Words Model** - A representation of text that describes the occurrence of words within a document. It is called a “bag” of words, because any information about the order or structure of words in the document is discarded. One of the limitations of Bag-of-Words approaches is that any information about the textual context surrounding that word is lost. The model is only concerned with whether known words occur in the document, not where in the document. \n",
        "\n",
        "BoW Involves:\n",
        "- A vocabulary of known words.\n",
        "- A measure of the presence of known words.\n",
        "\n",
        "One of the limitations of Bag-of-Words approaches is that any information about the textual context surrounding that word is lost. This also means that with bag-of-words approaches often the only tools that we have for identifying words with similar usage or meaning and subsequently consolidating them into a single vector is through the processes of stemming and lemmatization which tend to be quite limited at consolidating words unless the two words are very close in their spelling or in their root parts-of-speech."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik7vhXrJkWDc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Basic Bag-of-Words\n",
        "from collections import Counter\n",
        "\n",
        "df['tokens'] = df['reviews.text'].apply(tokenize)\n",
        "\n",
        "# The object `Counter` takes an iterable, \n",
        "# but you can instaniate an empty one and update it. \n",
        "word_counts = Counter()\n",
        "\n",
        "# Update it based on a split of each of our documents\n",
        "df['tokens'].apply(lambda x: word_counts.update(x))\n",
        "\n",
        "# Print out the 10 most common words\n",
        "word_counts.most_common(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il88N3NR4C4l",
        "colab_type": "text"
      },
      "source": [
        "**Term Frequency - Inverse Document Frequency (TF-IDF)** - The purpose of TF-IDF is to find what is unique to each document. Because of this we penalize the term frequencies of words that are common across all documents which will allow for each document's unique topics to rise to the top.\n",
        "- Term Frequency: Percentage of words in document for each word\n",
        "- Document Frequency: A penalty for the word existing in a high number of documents.\n",
        "<center><img src=\"https://mungingdata.files.wordpress.com/2017/11/equation.png?w=430&h=336\" width=\"300\"></center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLentD84r-IV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Term Frequency - Inverse Document Frequency\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Instantiate vectorizer object\n",
        "tfidf = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "\n",
        "# Create a vocabulary and get word counts per document\n",
        "dtm = tfidf.fit_transform(text)\n",
        "\n",
        "# View Feature Matrix as DataFrame\n",
        "docs = pd.DataFrame(dtm.todense(), columns = tfidf.get_feature_names())\n",
        "docs.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPdcXiyL_9Ov",
        "colab_type": "text"
      },
      "source": [
        "**Latent Semantic Indexing** - A technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts (topics) related to the documents and terms. LSA assumes that words that are close in meaning will occur in similar pieces of text. Values close to 1 represent very similar paragraphs while values close to 0 represent very dissimilar paragraphs.\n",
        "\n",
        "**Topic Modeling** - A type of statistical modeling for discovering the abstract “topics” that occur in a collection of documents. Latent Dirichlet Allocation (LDA) is an example of topic modeling and is used to classify text in a document to a particular topic. It builds a topic per document model and words per topic model, modeled as Dirichlet distributions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJ39A1LbkAB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LDA Modeling\n",
        "# A Dictionary Representation of all the words in our corpus\n",
        "id2word = corpora.Dictionary(doc_stream(path))\n",
        "\n",
        "# Let's remove extreme values from the dataset\n",
        "id2word.filter_extremes(no_below=10, no_above=0.75)\n",
        "\n",
        "corpus = [id2word.doc2bow(text) for text in doc_stream(path)]\n",
        "\n",
        "lda = LdaMulticore(corpus=corpus,\n",
        "                   id2word=id2word,\n",
        "                   random_state=723812,\n",
        "                   num_topics = 15,\n",
        "                   passes=10,\n",
        "                   workers=4\n",
        "                  )\n",
        "\n",
        "words = [re.findall(r'\"([^\"]*)\"',t[1]) for t in lda.print_topics()]\n",
        "topics = [' '.join(t[0:5]) for t in words]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLC-N7mvltCg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LDA Interpretation\n",
        "pyLDAvis.enable_notebook()\n",
        "pyLDAvis.gensim.prepare(lda, corpus, id2word)\n",
        "distro = [lda[d] for d in corpus]\n",
        "\n",
        "def update(doc):\n",
        "        d_dist = {k:0 for k in range(0,15)}\n",
        "        for t in doc:\n",
        "            d_dist[t[0]] = t[1]\n",
        "        return d_dist\n",
        "    \n",
        "new_distro = [update(d) for d in distro]\n",
        "\n",
        "df = pd.DataFrame.from_records(new_distro, index=titles)\n",
        "df.columns = topics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9BP3oU0meOP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_coherence_values(dictionary, corpus, path, limit, start=2, step=3):\n",
        "    \"\"\"\n",
        "    Compute c_v coherence for various number of topics\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    dictionary : Gensim dictionary\n",
        "    corpus : Gensim corpus\n",
        "    path : path to input texts\n",
        "    limit : Max num of topics\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    model_list : List of LDA topic models\n",
        "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
        "    \"\"\"\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        stream = doc_stream(path)\n",
        "        model = LdaMulticore(corpus=corpus, num_topics=num_topics, id2word=id2word, workers=4)\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=stream, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "\n",
        "    return model_list, coherence_values\n",
        "# Can take a long time to run.\n",
        "model_list, coherence_values = compute_coherence_values(dictionary=id2word, \n",
        "                                                        corpus=corpus, \n",
        "                                                        path=path, \n",
        "                                                        start=2, \n",
        "                                                        limit=40, \n",
        "                                                        step=6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbnyzZcNmpLD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Show graph\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "limit=40; start=2; step=6;\n",
        "x = range(start, limit, step)\n",
        "plt.plot(x, coherence_values)\n",
        "plt.xlabel(\"Num Topics\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.legend((\"coherence_values\"), loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "794-HlQ3ABH5",
        "colab_type": "text"
      },
      "source": [
        "###Other"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2VyonoW9i22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# KNN Example\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "nn = NearestNeighbors(n_neighbors=5, algorithm='ball_tree')\n",
        "\n",
        "# Fit on TF-IDF Vectors\n",
        "nn.fit(dtm.todense())\n",
        "\n",
        "# Query Using kneighbors \n",
        "nn.kneighbors(dtm.todense()[232])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORiPm-RssibB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# KNN with Outside Text Sample\n",
        "new = tfidf.transform(random_tech_article)\n",
        "nn.kneighbors(new.todense())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulUXqBKmndUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predicting categorical values with NLP\n",
        "vectorizer = TfidfVectorizer(stop_words='english', \n",
        "                        max_features = 10000)\n",
        "sgdc  = SGDClassifier()\n",
        "pipe = Pipeline([('vect', vectorizer), ('sgdc', sgdc)])\n",
        "pipe.fit(X, y)\n",
        "\n",
        "score = (cross_val_score(pipe, X, y, \n",
        "                          cv = 10, \n",
        "                          scoring = 'accuracy',\n",
        "                          n_jobs = -1,\n",
        "                          verbose = 10)).mean()\n",
        "print(score)\n",
        "\n",
        "preds = pipe.predict(test['description'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHlEc5cliUIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}