{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Learning Support Cheat Sheet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bundickm/CheatSheets/blob/master/Machine_Learning_Support_Cheat_Sheet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEFkJe4WsuXW",
        "colab_type": "text"
      },
      "source": [
        "#Resources\n",
        "[Cheat Sheets](https://github.com/bundickm/CheatSheets)\n",
        "- [Regression](https://github.com/bundickm/CheatSheets/blob/master/Classification_Validation_Cheat_Sheet.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk1tLgZ6tEvA",
        "colab_type": "text"
      },
      "source": [
        "##Universal Workflow of Machine Learning\n",
        "1. **Define the problem at hand and the data on which you’ll train.** Collect this data, or annotate it with labels if need be.\n",
        "\n",
        "2. **Choose how you’ll measure success on your problem.** Which [metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values) will you monitor on your validation data?\n",
        "\n",
        "3. **Determine your evaluation protocol.** Hold-out validation? K-fold validation? Which portion of the data should you use for validation?\n",
        "\n",
        "4. **Develop a first model that does better than a basic baseline:** a model with statistical power.\n",
        "\n",
        "5. **Develop a model that overfits.** The universal tension in machine learning is between optimization and generalization; the ideal model is one that stands right at the border between underfitting and overfitting; between undercapacity and overcapacity. To figure out where this border lies, first you must cross it.\n",
        "\n",
        "6. **Regularize your model and tune its hyperparameters, based on performance on the validation data.** Repeatedly modify your model, train it, evaluate on your validation data (not the test data, at this point), modify it again, and repeat, until the model is as good as it can get. Iterate on feature engineering: add new features, or remove features that don’t seem to be informative.\n",
        "\n",
        "Once you’ve developed a satisfactory model configuration, you can train your final production model on all the available data (training and validation) and evaluate it one last time on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ros2jxJCxVwC",
        "colab_type": "text"
      },
      "source": [
        "###Validation Curve\n",
        "Validation curves visualize the performance metric over a range of values for some hyperparameter.\n",
        "\n",
        "<center><img src= \"https://camo.githubusercontent.com/f89eaf0abb225cda2ab4beb8eee18d621d7cacf4/68747470733a2f2f6a616b657664702e6769746875622e696f2f507974686f6e44617461536369656e636548616e64626f6f6b2f666967757265732f30352e30332d76616c69646174696f6e2d63757276652e706e67\" width=400></center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3zPPJh5snD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import validation_curve\n",
        "\n",
        "model = RandomForestRegressor(n_estimators=100)\n",
        "\n",
        "depth = [2, 3, 4, 5, 6]\n",
        "train_score, val_score = validation_curve(\n",
        "    model, X_train, y_train,\n",
        "    param_name='max_depth', param_range=depth, \n",
        "    scoring='neg_mean_absolute_error', cv=3)\n",
        "\n",
        "plt.plot(depth, np.median(train_score, 1), color='blue', label='training score')\n",
        "plt.plot(depth, np.median(val_score, 1), color='red', label='validation score')\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel('depth');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9mza0gHyMDk",
        "colab_type": "text"
      },
      "source": [
        "##Hyperparameter Optimization\n",
        "\n",
        "**Hyperparameter Optimization** - The problem of choosing a set of optimal hyperparameters for a learning algorithm. The same kind of machine learning model can require different constraints, weights or learning rates to generalize different data patterns. These measures are called hyperparameters, and have to be tuned so that the model can optimally solve the machine learning problem. Hyperparameter optimization finds a tuple of hyperparameters that yields an optimal model which minimizes a predefined loss function on given independent data. The objective function takes a tuple of hyperparameters and returns the associated loss. Cross-validation is often used to estimate this generalization performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na6geKwmyRxc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "param_distributions = {\n",
        "    'n_estimators':[100,200],\n",
        "    'max_depth':[4,5],\n",
        "    'criterion':['mse','mae']\n",
        "}\n",
        "\n",
        "search = RandomizedSearchCV(\n",
        "    RandomForestRegressor(n_jobs=-1, random_state=42),\n",
        "    param_distributions=param_distributions, n_iter=8, cv=3,\n",
        "    scoring='neg_mean_absolute_error', verbose=10,\n",
        "    return_train_score=True, n_jobs=-1\n",
        ")\n",
        "\n",
        "search.fit(X_train, y_train)\n",
        "results = pd.DataFrame(search.cv_results_)\n",
        "results.sort_values(by='rank_test_score').head(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMCb_0t3yY-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "search.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpNpp6Gv3R_U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Graphing importances in descending order\n",
        "importances = pd.Series(search.best_estimator_.feature_importances_,X_train.columns)\n",
        "plt.figure(figsize=(5,10))\n",
        "importances.sort_values().plot.barh(color='grey');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olSyZrVg3htx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Final scoring on test data\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "final = search.best_estimator_\n",
        "y_pred = final.predict(X_test)\n",
        "test_mae = mean_absolute_error(y_test,y_pred)\n",
        "\n",
        "print('MAE with holdout test set:',test_mae)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KLpeUBdzitu",
        "colab_type": "text"
      },
      "source": [
        "##Feature Engineering\n",
        "**Feature engineering** - The process of using your own knowledge about the data and about the machine learning algorithm at hand to make the algorithm work better by applying hardcoded (non-learned) transformations to the data before it goes into the model. In many cases, it isn’t reasonable to expect a machine-learning model to be able to learn from completely arbitrary data. The data needs to be presented to the model in a way that will make the model’s job easier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwsiVZ1A5_yu",
        "colab_type": "text"
      },
      "source": [
        "#Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvqL91oY6dYY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pip installs\n",
        "!pip install eli5\n",
        "!pip install pdpbox\n",
        "!pip install shap"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK5GY7dC6pBq",
        "colab_type": "text"
      },
      "source": [
        "##Feature Importances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-mGRG9I5_WB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Feature Importances\n",
        "  #best is the best model from RandomizedSearchCV\n",
        "n = 20\n",
        "figsize = (5,n//3)\n",
        "\n",
        "importances = pd.Series(best.feature_importances_, X_train.columns)\n",
        "top_n = importances.sort_values()[-n:]\n",
        "\n",
        "plt.figure(figsize=figsize)\n",
        "top_n.plot.barh(color='gray');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03HoSL_a6rro",
        "colab_type": "text"
      },
      "source": [
        "##Permutation Importances\n",
        "Permutation Importance is a compromise between Feature Importance based on impurity reduction (which is the fastest) and Drop Column Importance (which is the \"best.\")\n",
        "\n",
        "The ELI5 library documentation explains,\n",
        "\n",
        "    Importance can be measured by looking at how much the score (accuracy, F1, R^2, etc. - any score we’re interested in) decreases when a feature is not available.\n",
        "\n",
        "    To do that one can remove feature from the dataset, re-train the estimator and check the score. But it requires re-training an estimator for each feature, which can be computationally intensive. ...\n",
        "\n",
        "    To avoid re-training the estimator we can remove a feature only from the test part of the dataset, and compute score without using this feature. It doesn’t work as-is, because estimators expect feature to be present. So instead of removing a feature we can replace it with random noise - feature column is still there, but it no longer contains useful information. This method works if noise is drawn from the same distribution as original feature values (as otherwise estimator may fail). The simplest way to get such noise is to shuffle values for a feature, i.e. use other examples’ feature values - this is how permutation importance is computed.\n",
        "\n",
        "    The method is most suitable for computing feature importances when a number of columns (features) is not huge; it can be resource-intensive otherwise.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0Xs8mKy6ZQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#table of feature importances via permutation importance\n",
        "import eli5\n",
        "from eli5.sklearn import PermutationImportance\n",
        "\n",
        "permuter = PermutationImportance(best, scoring='roc_auc', cv='prefit', \n",
        "                                 n_iter=2, random_state=42)\n",
        "\n",
        "permuter.fit(X_test.values, y_test)\n",
        "\n",
        "feature_names = X_test.columns.tolist()\n",
        "eli5.show_weights(permuter, top=None, feature_names=feature_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyDDUCF67VQM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dropping features with 0 importance for faster training but almost equal score\n",
        "mask = permuter.feature_importances_ > 0\n",
        "features = X_train.columns[mask]\n",
        "X_train = X_train[features]\n",
        "\n",
        "###Refit the model after removing features###"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxMDR3pD73DR",
        "colab_type": "text"
      },
      "source": [
        "##Partial Dependence Plots\n",
        "\n",
        "When using black box machine learning algorithms like random forest and boosting, it is hard to understand the relations between predictors and model outcome. For example, in terms of random forest, all we get is the feature importance. Although we can know which feature is significantly influencing the outcome based on the importance calculation, we don’t know in which direction it is influencing. And in most cases, the effect is non-monotonic. We need tools to help understanding of the complex relations between predictors and model prediction.\n",
        "\n",
        "Partial dependence plots show how a feature affects predictions of a Machine Learning model on average.\n",
        "1. Define grid along feature\n",
        "2. Model predictions at grid points\n",
        "3. Line per data instance -> ICE (Individual Conditional Expectation) curve\n",
        "4. Average curves to get a PDP (Partial Dependence Plot)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyV74QRm72gA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#single feature dependence plot\n",
        "from pdpbox.pdp import pdp_isolate, pdp_plot\n",
        "\n",
        "feature = 'sub_grade'\n",
        "\n",
        "isolated = pdp_isolate(model=best, dataset=X_test, \n",
        "                       model_features=X_test.columns, feature=feature)\n",
        "\n",
        "pdp_plot(isolated, feature_name=feature);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeyiv2m38nIQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#2 feature interaction partial dependence plot (heatmap)\n",
        "from pdpbox.pdp import pdp_interact, pdp_interact_plot\n",
        "\n",
        "features =['sub_grade','dti']\n",
        "\n",
        "interaction = pdp_interact(model=best, dataset=X_test, \n",
        "                           model_features=X_test.columns, features=features)\n",
        "\n",
        "pdp_interact_plot(interaction, plot_type='grid', feature_names=features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__I_Esx-8_zj",
        "colab_type": "text"
      },
      "source": [
        "##Shapley Values\n",
        "\n",
        "SHAP Values (an acronym from SHapley Additive exPlanations) break down a prediction to show the impact of each feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RH-O_42P9NoF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#SHAP Values diagram\n",
        "import shap\n",
        "shap.initjs()\n",
        "\n",
        "#this is just any entry, may be true positive, false negative, etc.\n",
        "data_for_prediction = X_test[X_test.index==91777] \n",
        "\n",
        "explainer = shap.TreeExplainer(best)\n",
        "shap_values = explainer.shap_values(data_for_prediction)\n",
        "shap.force_plot(explainer.expected_value, shap_values, data_for_prediction)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLLM0KzR94rm",
        "colab_type": "text"
      },
      "source": [
        "#Gradient Descent\n",
        "**Gradient descent** - an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model. Parameters refer to coefficients in Linear Regression and weights in neural networks.\n",
        "\n",
        "1.   Define a Cost Function\n",
        "2.   Evaluate slope (gradient) at current point\n",
        "3.   Take small step (alpha or learning rate) in direction of slope descent\n",
        "4.   Repeat steps 2 and 3 until slope approaches 0\n",
        "\n"
      ]
    }
  ]
}